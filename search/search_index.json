{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"K8sGPT Documentation","text":"<p>K8sGPT gives Kubernetes SRE superpowers to everyone</p> <p>The documentation provides the following</p> <ul> <li>Getting started: Guides to install and use K8sGPT</li> <li>Tutorials: End-to-end tutorials on specific use cases</li> <li>Reference: Specific documentation on the features</li> <li>Explanation: Additional explanations on the design and use of the CLI</li> </ul>"},{"location":"#documentation-enhancements","title":"Documentation enhancements","text":"<p>If anything is unclear please create an issue in the docs repository.</p>"},{"location":"explanation/caching/","title":"Caching","text":"<p>Remote caching is a mechanism used to store and retrieve frequently accessed data in a location separate from the primary system. In the context of <code>K8sGPT</code>, it allows users to offload cached data to a remote storage service, like AWS S3, rather than managing it on the local machine. This approach offers several benefits, such as reducing local storage requirements and ensuring cache persistence even when the local environment is updated or restarted.</p>"},{"location":"explanation/caching/#aws-s3-integration","title":"AWS S3 Integration","text":"<p>K8sGPT provides seamless integration with AWS S3, a widely adopted and reliable object storage service offered by Amazon Web Services. By leveraging this integration, users can take advantage of AWS S3's scalability, durability, and availability to store their cached data remotely.</p>"},{"location":"explanation/caching/#prerequisites","title":"Prerequisites","text":"<p>To use the remote caching feature with AWS S3 in K8sGPT, you need to have the following prerequisites set up:</p> <ul> <li> <p><code>AWS_ACCESS_KEY_ID</code>: An access key ID is required to authenticate with AWS S3 programmatically.</p> </li> <li> <p><code>AWS_SECRET_ACCESS_KEY</code>: The corresponding secret access key that pairs with the AWS access key ID.</p> </li> </ul> <p>Adding a Remote Cache:</p> <p>Users can easily add a remote cache to the K8sGPT CLI by executing the following command:</p> <pre><code>k8sgpt cache add --region &lt;aws region&gt; --bucket &lt;name&gt;\n</code></pre> <p>The command above will create a new bucket in the specified AWS region if it does not already exist. The created bucket will serve as the remote cache for K8sGPT.</p> <p>Listing Cache Items:</p> <p>To view the items stored in the remote cache, users can use the following command:</p> <pre><code>k8sgpt cache list\n</code></pre> <p>Removing the Remote Cache:</p> <p>If users wish to remove the remote cache without deleting the associated AWS S3 bucket, they can use the following command:</p> <pre><code>k8sgpt cache remove --bucket &lt;name&gt;\n</code></pre> <p>This command ensures that the cache items are removed from the K8sGPT CLI, but the bucket and its contents in AWS S3 will remain intact for potential future usage.</p>"},{"location":"explanation/integrations/","title":"Integrations","text":"<p>Integrations in k8sGPT allows you to manage and configure various integrations with external tools and services within your repository's codebase.</p> <p>These integrations enhance the functionality of k8sGPT by providing additional capabilities for scanning, diagnosing, and triaging issues in the Kubernetes clusters.</p>"},{"location":"explanation/integrations/#description","title":"Description","text":"<p>The <code>integration</code> command in the k8sgpt enables seamless integration with external tools and services. It allows you to activate, configure, and manage integrations that complement the functionalities of k8sgpt.</p> <p>Integrations are designed to interact with external systems and tools that complement the functionalities of k8sgpt. These integrations include vulnerability scanners, monitoring services, incident management platforms, and more.</p> <p>By using the following command users can access all K8sGPT CLI options related to integrations:</p> <pre><code>k8sgpt integrations --help\n</code></pre> <p>By leveraging the <code>integration</code> feature in the K8sGPT CLI, users can extend the functionality of K8sGPT by incorporating various external tools and services. This collaboration enhances the ability to diagnose and triage issues in Kubernetes clusters more effectively.</p> <p>For more information about each <code>integration</code> and its specific configuration options, refer to the reference documentation provided for the integration.</p>"},{"location":"getting-started/Community/","title":"Community","text":""},{"location":"getting-started/Community/#github","title":"GitHub","text":"<p>The k8sgpt source code and other related projects managed on github are in the k8sgpt organization.</p>"},{"location":"getting-started/Community/#slack-channel","title":"Slack Channel","text":"<p>You can join the slack channel using the  link : slack</p>"},{"location":"getting-started/Community/#community-meetings-office-hours","title":"Community Meetings / Office Hours","text":"<p>These happen on 1st and 3rd Thursday of the month Time zone: Europe/London Time: 12:00 - 13:00 Joining Info:</p> <p>Google Meet : link</p> <p>Calendar Link : calendar schedule</p>"},{"location":"getting-started/Community/#contributing","title":"Contributing","text":"<p>Thanks for your interest in contributing! We welcome all types of contributions and encourage you to read our contribution guidelines for next steps.</p>"},{"location":"getting-started/getting-started/","title":"Getting Started Guide","text":"<p>You can either get started with K8sGPT in your own environment, the details are provided below or you can use our Playground example on Killercoda.</p> <p>Tip</p> <p>Please only use K8sGPT on environments where you are authorized to modify Kubernetes resources.</p>"},{"location":"getting-started/getting-started/#prerequisites","title":"Prerequisites","text":"<ol> <li>Ensure <code>k8sgpt</code> is installed correctly on your environment by following the installation.</li> <li>You need to be connected to any Kubernetes cluster. Below is the documentation for setting up a new KinD Kubernetes cluster. However, make sure that kubectl is already installed.</li> </ol>"},{"location":"getting-started/getting-started/#setting-up-a-kubernetes-cluster","title":"Setting up a Kubernetes cluster","text":"<p>To give <code>k8sgpt</code> a try, set up a basic Kubernetes cluster, such as KinD or Minikube (if you are not connected to any other cluster).</p> <ul> <li> <p>The KinD documentation provides several installation options to set up a local cluster with two commands.</p> </li> <li> <p>The Minikube documentation covers different Operating Systems and Architectures to set up a local Kubernetes cluster running on a Container or Virtual Machine.</p> </li> </ul> <p>Creating a KinD Kubernetes Cluster</p> <p>Install KinD first:</p> <pre><code>brew install kind\n</code></pre> <p>Create a new Kubernetes cluster:</p> <pre><code>kind create cluster --name k8sgpt-demo\n</code></pre>"},{"location":"getting-started/getting-started/#using-k8sgpt","title":"Using K8sGPT","text":"<p>You can view the different command options through </p> <pre><code>k8sgpt --help\nKubernetes debugging powered by AI\n\nUsage:\n  k8sgpt [command]\n\nAvailable Commands:\n  analyze     This command will find problems within your Kubernetes cluster\n  auth        Authenticate with your chosen backend\n  cache       For working with the cache the results of an analysis\n  completion  Generate the autocompletion script for the specified shell\n  custom-analyzer Manage a custom analyzer\n  dump            Creates a dumpfile for debugging issues with K8sGPT\n  filters     Manage filters for analyzing Kubernetes resources\n  generate    Generate Key for your chosen backend (opens browser)\n  help        Help about any command\n  integration Integrate another tool into K8sGPT\n  serve       Runs k8sgpt as a server\n  version     Print the version number of k8sgpt\n\nFlags:\n      --config string        Default config file (default is $HOME/.k8sgpt.yaml)\n  -h, --help                 help for k8sgpt\n      --kubeconfig string    Path to a kubeconfig. Only required if out-of-cluster.\n      --kubecontext string   Kubernetes context to use. Only required if out-of-cluster.\n\nUse \"k8sgpt [command] --help\" for more information about a command.\n</code></pre>"},{"location":"getting-started/getting-started/#authenticate-with-openai","title":"Authenticate with OpenAI","text":"<p>First, you will need to authenticate with your chosen backend. The backend is the AI provider such as OpenAI's ChatGPT.</p> <p>Ensure that you have created an account with OpenAI.</p> <p>Next, generate a token from the backend:</p> <pre><code>k8sgpt generate\n</code></pre> <p>This will provide you with a URL to generate a token, follow the URL from the command line to your browser to then generate the token.</p> <p></p> <p>Copy the token for the next step.</p> <p>Then, authenticate with the following command:</p> <pre><code>k8sgpt auth add --backend openai --model gpt-4o-mini\n</code></pre> <p>This will request the token that has just been generated. Paste the token into the command line.</p> <p>You should then see the following success message:</p> <p>Enter openai Key: openai added to the AI backend provider list</p>"},{"location":"getting-started/getting-started/#analyze-your-cluster","title":"Analyze your cluster","text":"<p>Ensure that you are connected the correct Kubernetes cluster, for this initial example is preferable to use KinD or Minikube as discussed earlier.</p> <pre><code>kubectl config current-context\n</code></pre> <pre><code>kubectl get nodes\n</code></pre> <p>We will now create a new \"broken Pod\", simply create a new YAML file named <code>broken-pod.yml</code> with the following contents:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: broken-pod\n  namespace: default\nspec:\n  containers:\n    - name: broken-pod\n      image: nginx:1.a.b.c\n      livenessProbe:\n        httpGet:\n          path: /\n          port: 81\n        initialDelaySeconds: 3\n        periodSeconds: 3\n</code></pre> <p>You might have noticed, this Pod has a wrong image tag. This is ok for this example, we simply want to have an issue in our cluster. The simply run:</p> <pre><code>kubectl apply -f broken-pod.yml\n</code></pre> <p>This will create the \"broken Pod\" in the cluster. You can verify this by running:</p> <pre><code>kubectl get pods\n\nNAME         READY   STATUS         RESTARTS   AGE\nbroken-pod   0/1     ErrImagePull   0          5s\n</code></pre> <p>Now, you can go ahead and analyse your cluster:</p> <pre><code>k8sgpt analyze\n</code></pre> <p>Executing this command will generate a list of issues present in your Kubernetes cluster. In the case of our example, a message should be displayed highlighting the problem related to the container image.</p> <pre><code>0 default/broken-pod(broken-pod)\n- Error: Back-off pulling image \"nginx:1.a.b.c\"\n</code></pre> <p>Info</p> <p>To become acquainted with the available flags supported by the <code>analyse</code> command, type <code>k8sgpt analyse -h</code> for more information. This will provide you with a comprehensive list of all the flags that can be utilized.</p> <p>For a more engaging experience and a better understanding of the capabilities of <code>k8sgpt</code> and LLMs (Large Language Models), run the following command:</p> <pre><code>k8sgpt analyse --explain\n</code></pre> <p>Congratulations! you have successfully created a local kubernetes cluster, deployed a \"broken Pod\" and analyzed it using <code>k8sgpt</code>.</p>"},{"location":"getting-started/in-cluster-operator/","title":"K8sGPT Operator","text":""},{"location":"getting-started/in-cluster-operator/#prerequisites","title":"Prerequisites","text":"<ul> <li>To begin you will require a Kubernetes cluster available and <code>KUBECONFIG</code> set.</li> <li>You will also need to install helm v3. See the Helm documentation for more information.</li> </ul>"},{"location":"getting-started/in-cluster-operator/#operator-installation","title":"Operator Installation","text":"<p>To install the operator, run the following command:</p> <pre><code>helm repo add k8sgpt https://charts.k8sgpt.ai/\nhelm repo update\nhelm install release k8sgpt/k8sgpt-operator -n k8sgpt-operator-system --create-namespace\n</code></pre> <p>This will install the Operator into the cluster, which will await a <code>K8sGPT</code> resource before anything happens.</p>"},{"location":"getting-started/in-cluster-operator/#deploying-an-openai-secret","title":"Deploying an OpenAI secret","text":"<p>Whilst there are multiple backends supported ( OpenAI, Azure OpenAI and Local ), in this example we'll use OpenAI. Whatever backend you are using, you need to make sure to have a secret that works with the backend.</p> <p>For instance, this means you will need to install your OpenAI token as a secret into the cluster:</p> <pre><code>kubectl create secret generic k8sgpt-sample-secret --from-literal=openai-api-key=$OPENAI_TOKEN -n k8sgpt-operator-system\n</code></pre>"},{"location":"getting-started/in-cluster-operator/#deploying-a-k8sgpt-resource","title":"Deploying a K8sGPT resource","text":"<p>To deploy a K8sGPT resource, you will need to create a YAML file with the following contents:</p> <pre><code>kubectl apply -f - &lt;&lt; EOF\napiVersion: core.k8sgpt.ai/v1alpha1\nkind: K8sGPT\nmetadata:\n  name: k8sgpt-sample\n  namespace: k8sgpt-operator-system\nspec:\n  ai:\n    enabled: true\n    model: gpt-4o-mini\n    backend: openai\n    secret:\n      name: k8sgpt-sample-secret\n      key: openai-api-key\n    # anonymized: false\n    # language: english\n  noCache: false\n  version: v0.3.41\n  # filters:\n  #   - Ingress\n  # sink:\n  #   type: slack\n  #   webhook: &lt;webhook-url&gt;\n  # extraOptions:\n  #   backstage:\n  #     enabled: true\nEOF\n</code></pre> <p>Please replace the <code>&lt;VERSION&gt;</code> field with the current release of K8sGPT. At the time of writing this is <code>v0.3.41</code>.</p>"},{"location":"getting-started/in-cluster-operator/#regarding-out-of-cluster-traffic-to-ai-backends","title":"Regarding out-of-cluster traffic to AI backends","text":"<p>In the above example <code>enableAI</code> is set to <code>true</code>. This option allows the cluster deployment to use the <code>backend</code> to filter and improve the responses to the user. Those responses will appear as <code>details</code> within the <code>Result</code> custom resources that are created.</p> <p>The default backend in this example is OpenAI and allows for additional details to be generated and solutions provided for issues. If you wish to disable out-of-cluster communication and any Artificial Intelligence processing through models, simply set <code>enableAI</code> to <code>false</code>.</p> <p>It should also be noted that <code>localai</code> and <code>azureopenai</code> are supported and in-cluster models will be supported in the near future</p>"},{"location":"getting-started/in-cluster-operator/#viewing-the-results","title":"Viewing the results","text":"<p>Once the initial scans have been completed after several minutes, you will be presented with results custom resources.</p> <pre><code>\u276f kubectl get results -o json -n k8sgpt-operator-system| jq .\n{\n  \"apiVersion\": \"v1\",\n  \"items\": [\n    {\n      \"apiVersion\": \"core.k8sgpt.ai/v1alpha1\",\n      \"kind\": \"Result\",\n      \"metadata\": {\n        \"creationTimestamp\": \"2023-04-26T09:45:02Z\",\n        \"generation\": 1,\n        \"name\": \"placementoperatorsystemplacementoperatorcontrollermanagermetricsservice\",\n        \"namespace\": \"k8sgpt-operator-system\",\n        \"resourceVersion\": \"108371\",\n        \"uid\": \"f0edd4de-92b6-4de2-ac86-5bb2b2da9736\"\n      },\n      \"spec\": {\n        \"details\": \"The error message means that the service in Kubernetes doesn't have any associated endpoints, which should have been labeled with \\\"control-plane=controller-manager\\\". \\n\\nTo solve this issue, you need to add the \\\"control-plane=controller-manager\\\" label to the endpoint that matches the service. Once the endpoint is labeled correctly, Kubernetes can associate it with the service, and the error should be resolved.\",\n        ...\n</code></pre>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>This page provides further information on installation guidelines.</p>"},{"location":"getting-started/installation/#linuxmac-via-brew","title":"Linux/Mac via brew","text":""},{"location":"getting-started/installation/#prerequisites","title":"Prerequisites","text":"<p>Ensure that you have Homebrew installed:</p> <ul> <li>Homebrew for Mac</li> <li>Homebrew for Linux Homebrew for Linux also works on WSL</li> </ul>"},{"location":"getting-started/installation/#homebrew","title":"Homebrew","text":"<p>Install K8sGPT on your machine with the following commands:</p> <pre><code>brew tap k8sgpt-ai/k8sgpt\nbrew install k8sgpt\n</code></pre>"},{"location":"getting-started/installation/#other-installation-options","title":"Other Installation Options","text":""},{"location":"getting-started/installation/#rpm-based-installation-redhatcentosfedora","title":"RPM-based installation (RedHat/CentOS/Fedora)","text":"<p>32 bit:</p> <pre><code>curl -LO https://github.com/k8sgpt-ai/k8sgpt/releases/download/v0.3.24/k8sgpt_386.rpm\nsudo rpm -ivh k8sgpt_386.rpm\n</code></pre> <p>64 bit:</p> <pre><code>curl -LO https://github.com/k8sgpt-ai/k8sgpt/releases/download/v0.3.24/k8sgpt_amd64.rpm\nsudo rpm -ivh -i k8sgpt_amd64.rpm\n</code></pre>"},{"location":"getting-started/installation/#deb-based-installation-ubuntudebian","title":"DEB-based installation (Ubuntu/Debian)","text":"<p>32 bit:</p> <pre><code>curl -LO https://github.com/k8sgpt-ai/k8sgpt/releases/download/v0.3.24/k8sgpt_386.deb\nsudo dpkg -i k8sgpt_386.deb\n</code></pre> <p>64 bit:</p> <pre><code>curl -LO https://github.com/k8sgpt-ai/k8sgpt/releases/download/v0.3.24/k8sgpt_amd64.deb\nsudo dpkg -i k8sgpt_amd64.deb\n</code></pre>"},{"location":"getting-started/installation/#apk-based-installation-alpine","title":"APK-based installation (Alpine)","text":"<p>32 bit:</p> <pre><code>curl -LO https://github.com/k8sgpt-ai/k8sgpt/releases/download/v0.3.24/k8sgpt_386.apk\napk add k8sgpt_386.apk\n</code></pre> <p>64 bit:</p> <pre><code>curl -LO https://github.com/k8sgpt-ai/k8sgpt/releases/download/v0.3.24/k8sgpt_amd64.apk\napk add k8sgpt_amd64.apk\n</code></pre>"},{"location":"getting-started/installation/#windows","title":"Windows","text":"<ul> <li>Download the latest Windows binaries of k8sgpt from the Release   tab based on your system architecture.</li> <li>Extract the downloaded package to your desired location. Configure the system path variable with the binary location.</li> </ul>"},{"location":"getting-started/installation/#verify-installation","title":"Verify installation","text":"<p>Verify that K8sGPT is installed correctly:</p> <pre><code>k8sgpt version\n\nk8sgpt version 0.2.7\n</code></pre>"},{"location":"getting-started/installation/#common-issues","title":"Common Issues","text":""},{"location":"getting-started/installation/#windows-wsl","title":"Windows WSL","text":"<p>Failing Installation on WSL or Linux (missing gcc) When installing Homebrew on WSL or Linux, you may encounter the following error:</p> <pre><code>==&gt; Installing k8sgpt from k8sgpt-ai/k8sgpt Error: The following formula cannot be installed from bottle and must be\nbuilt from source. k8sgpt Install Clang or run brew install gcc.\n</code></pre> <p>If you install gcc as suggested, the problem will persist. Therefore, you need to install the build-essential package.</p> <pre><code>   sudo apt-get update\n   sudo apt-get install build-essential\n</code></pre>"},{"location":"getting-started/installation/#failing-installation-on-wsl-or-linux-missing-gcc","title":"Failing Installation on WSL or Linux (missing gcc)","text":"<p>When installing Homebrew on WSL or Linux, you may encounter the following error:</p> <p><code>==&gt; Installing k8sgpt from k8sgpt-ai/k8sgpt Error: The following formula cannot be installed from a bottle and must be   built from the source. k8sgpt Install Clang or run brew install gcc.</code></p> <p>If you install gcc as suggested, the problem will persist. Therefore, you need to install the build-essential package.   <code>bash      sudo apt-get update      sudo apt-get install build-essential</code></p>"},{"location":"getting-started/installation/#running-k8sgpt-through-a-container","title":"Running K8sGPT through a container","text":"<p>If you are running K8sGPT through a container, the CLI will not be able to open the website for the OpenAI token.</p> <p>You can find the latest container image for K8sGPT in the packages of the GitHub organisation: Link</p> <p>A volume can then be mounted to the image through e.g. Docker Compose. Below is an example:</p> <pre><code>version: '2'\nservices:\n k8sgpt:\n   image: ghcr.io/k8sgpt-ai/k8sgpt:dev-202304011623\n   volumes:\n     -  /home/$(whoami)/.k8sgpt.yaml:/home/root/.k8sgpt.yaml\n</code></pre>"},{"location":"getting-started/installation/#installing-the-k8sgpt-operator-helm-chart","title":"Installing the K8sGPT Operator Helm Chart","text":"<p>K8sGPT can be installed as an Operator inside the cluster. For further information, see the K8sGPT Operator documentation.</p>"},{"location":"getting-started/installation/#installing-the-k8sgpt-operator-via-glasskube","title":"Installing the K8sGPT Operator via Glasskube","text":"<p>Glasskube is a Kubernetes package manager that simplifies the installation process of the k8sgpt-operator and automatically ensures it stays up-to-date with the latest version. For detailed instructions on installing Glasskube, refer to the Glasskube Installation.</p> <p>Install k8sgpt via the CLI:</p> <pre><code>glasskube install k8sgpt-operator --value \"openaiApiKey=&lt;openApiKeyValue&gt;\"\n</code></pre> <p>Alternatively, configure the package via the Glasskube UI, where you can easily customize the operator to anonymize data, choose the output language, and define the OpenAI API key seamlessly.</p>"},{"location":"getting-started/installation/#upgrading-the-brew-installation","title":"Upgrading the brew installation","text":"<p>To upgrade the K8sGPT brew installation run the following command:</p> <pre><code>brew upgrade k8sgpt\n</code></pre>"},{"location":"reference/cli/","title":"CLI Reference","text":"<p>This section provides an overview of the different <code>k8sgpt</code> CLI commands.</p> <p>Prerequisites</p> <ul> <li>You need to be connected to a Kubernetes cluster, K8sGPT will access it through your kubeconfig.</li> <li>Signed-up to OpenAI ChatGPT</li> <li>Have the K8sGPT CLI installed</li> </ul>"},{"location":"reference/cli/#commands","title":"Commands","text":"<p>Run a scan with the default analyzers</p> <pre><code>k8sgpt generate\nk8sgpt auth add\nk8sgpt analyze --explain\n</code></pre> <p>Filter on resource</p> <pre><code>k8sgpt analyze --explain --filter=Service\n</code></pre> <p>Filter by namespace</p> <pre><code>k8sgpt analyze --explain --filter=Pod --namespace=default\n</code></pre> <p>Output to JSON</p> <pre><code>k8sgpt analyze --explain --filter=Service --output=json\n</code></pre> <p>Anonymize during explain</p> <pre><code>k8sgpt analyze --explain --filter=Service --output=json --anonymize\n</code></pre>"},{"location":"reference/cli/#additional-commands","title":"Additional commands","text":"<p>List configured backends</p> <pre><code>k8sgpt auth list\n</code></pre> <p>Remove configured backends</p> <pre><code>k8sgpt auth remove --backend $MY_BACKEND\n</code></pre> <p>List integrations</p> <pre><code>k8sgpt integrations list\n</code></pre> <p>Activate integrations</p> <pre><code>k8sgpt integrations activate [integration(s)]\n</code></pre> <p>Use integration</p> <pre><code>k8sgpt analyze --filter=[integration(s)]\n</code></pre> <p>Deactivate integrations</p> <pre><code>k8sgpt integrations deactivate [integration(s)]\n</code></pre> <p>Serve mode with GRPC</p> <pre><code>k8sgpt serve\n</code></pre> <p>Analysis with GRPC serve mode</p> <pre><code>grpcurl -plaintext localhost:8080 schema.v1.ServerService/Analyze\n</code></pre> <p>Serve mode with GRPC and non-default backend (amazonbedrock)</p> <pre><code>k8sgpt serve -b amazonbedrock\n</code></pre> <p>Analysis with GRPC serve mode and non-default backend (amazonbedrock)</p> <pre><code>grpcurl -plaintext -d '{\"explain\": true, \"backend\": \"amazonbedrock\"}' localhost:8080 schema.v1.ServerService/Analyze\n</code></pre> <p>Serve mode with REST API</p> <pre><code>k8sgpt serve --http\n</code></pre> <p>Analysis with REST API serve mode</p> <pre><code>curl -X POST \"http://localhost:8080/v1/analyze\"\n</code></pre> <p>Serve mode with REST API serve mode and non-default backend (amazonbedrock)</p> <pre><code>k8sgpt serve --http -b amazonbedrock\n</code></pre> <p>Analysis with REST API serve mode and non-default backend (amazonbedrock)</p> <pre><code>curl -X POST \"http://localhost:8080/v1/analyze?explain=true&amp;backend=amazonbedrock\"\n</code></pre>"},{"location":"reference/cli/debugging/","title":"Debugging K8sGPT","text":"<p>If you are experiencing issues that you believe are related to K8sGPT. Please cut us an issue here and upload your K8sGPT dump file.</p> <p>To create a K8sGPT dump file run <code>k8sgpt dump</code>. This will create a <code>dump_&lt;time&gt;_json</code> file which you can attach to your github issue.</p> <pre><code>\u276f cat dump_20241112200820.json\n{\n \"AIConfiguration\": {\n  \"Providers\": [\n   {\n    \"Name\": \"openai\",\n    \"Model\": \"gpt-3.5-turbo\",\n    \"Password\": \"sk-p***\",\n    \"BaseURL\": \"\",\n    \"ProxyEndpoint\": \"\",\n    \"ProxyPort\": \"\",\n    \"EndpointName\": \"\",\n    \"Engine\": \"\",\n    \"Temperature\": 0.7,\n    \"ProviderRegion\": \"\",\n    \"ProviderId\": \"\",\n    \"CompartmentId\": \"\",\n    \"TopP\": 0.5,\n    \"TopK\": 50,\n    \"MaxTokens\": 2048,\n    \"OrganizationId\": \"\",\n    \"CustomHeaders\": []\n   }\n  ],\n  \"DefaultProvider\": \"\"\n },\n \"ActiveFilters\": [\n  \"Deployment\",\n  \"StatefulSet\",\n  \"ReplicaSet\",\n  \"Ingress\",\n  \"ValidatingWebhookConfiguration\",\n  \"PersistentVolumeClaim\",\n  \"CronJob\",\n  \"MutatingWebhookConfiguration\",\n  \"Gateway\"\n ],\n \"KubenetesServerVersion\": {\n  \"major\": \"1\",\n  \"minor\": \"31\",\n  \"gitVersion\": \"v1.31.0\",\n  \"gitCommit\": \"9edcffcde5595e8a5b1a35f88c421764e575afce\",\n  \"gitTreeState\": \"clean\",\n  \"buildDate\": \"2024-08-13T07:28:49Z\",\n  \"goVersion\": \"go1.22.5\",\n  \"compiler\": \"gc\",\n  \"platform\": \"linux/amd64\"\n },\n \"K8sGPTInfo\": {\n  \"Version\": \"dev\",\n  \"Commit\": \"HEAD\",\n  \"Date\": \"unknown\"\n }\n}%\n</code></pre>"},{"location":"reference/cli/filters/","title":"Using Integration and Filters in K8sGPT","text":"<p>K8sGPT offers integration with other tools. Once an integration is added to K8sGPT, it is possible to use its resources as additional filters.</p> <ul> <li>Filters are a way of selecting which resources you wish to be part of your default analysis.</li> <li>Integrations are a way to add resources to the filter list.</li> </ul> <p>Use the following command to access all K8sGPT CLI options related to integrations:</p> <pre><code>k8sgpt integrations\n</code></pre>"},{"location":"reference/cli/filters/#prerequisites","title":"Prerequisites","text":"<p>For using the K8sGPT integrations please ensure that you have the latest version of the K8sGPT CLI installed. Also, please make sure that you are connected to a Kubernetes cluster.</p>"},{"location":"reference/cli/filters/#activating-an-integration","title":"Activating an Integration","text":"<p>Prerequisites</p> <ul> <li>Connected to a running Kubernetes cluster, any cluster will work for demonstration purposes.</li> </ul> <p>To list all integrations run the following command:</p> <pre><code>k8sgpt integrations list\n</code></pre> <p>This will provide you with a list of available integrations.</p>"},{"location":"reference/cli/filters/#trivy","title":"Trivy","text":"<p>The first integration that has been added is Trivy. Trivy is an open source, cloud native security scanner, maintained by Aqua Security.</p> <p>Activate the Trivy integration:</p> <pre><code>k8sgpt integration activate trivy\n</code></pre> <p>Once activated, you should see the following success message displayed:</p> <pre><code>Activated integration trivy\n</code></pre> <p>This will install the Trivy Kubernetes Operator into the Kubernetes cluster and make it possible for K8sGPT to interact with the results of the Operator.</p> <p>Once the Trivy Operator is installed inside the cluster, K8sGPT will have access to VulnerabilityReports and ConfigAuditReports:</p> <pre><code>\u276f k8sgpt filters list\n\nActive:\n&gt; VulnerabilityReport (integration)\n&gt; Pod\n&gt; ConfigAuditReport (integration)\nUnused:\n&gt; PersistentVolumeClaim\n&gt; Service\n&gt; CronJob\n&gt; Node\n&gt; MutatingWebhookConfiguration\n&gt; Deployment\n&gt; StatefulSet\n&gt; ValidatingWebhookConfiguration\n&gt; ReplicaSet\n&gt; Ingress\n&gt; HorizontalPodAutoScaler\n&gt; PodDisruptionBudget\n&gt; NetworkPolicy\n</code></pre> <p>More information can be found on the official Trivy-Operator documentation.</p>"},{"location":"reference/cli/filters/#using-the-new-filters-to-analyze-your-cluster","title":"Using the new filters to analyze your cluster","text":"<p>Any of the filters listed in the previous section can be used as part of the <code>k8sgpt analyze</code> command.</p> <p>To use the <code>VulnerabilityReport</code> filter from the Trivy integration, set it through the <code>--filter</code> flag:</p> <pre><code>k8sgpt analyze --filter VulnerabilityReport\n</code></pre> <p>This command will analyze your cluster Vulnerabilities through K8sGPT. Depending on the VulnerabilityReports available in your cluster, the result of the report will look different:</p> <pre><code>\u276f k8sgpt analyze --filter VulnerabilityReport\n\n0 demo/nginx-deployment-7bcfc88bbf(Deployment/nginx-deployment)\n- Error: critical Vulnerability found ID: CVE-2023-23914 (learn more at: https://avd.aquasec.com/nvd/cve-2023-23914)\n- Error: critical Vulnerability found ID: CVE-2023-27536 (learn more at: https://avd.aquasec.com/nvd/cve-2023-27536)\n- Error: critical Vulnerability found ID: CVE-2023-23914 (learn more at: https://avd.aquasec.com/nvd/cve-2023-23914)\n- Error: critical Vulnerability found ID: CVE-2023-27536 (learn more at: https://avd.aquasec.com/nvd/cve-2023-27536)\n- Error: critical Vulnerability found ID: CVE-2019-8457 (learn more at: https://avd.aquasec.com/nvd/cve-2019-8457)\n</code></pre>"},{"location":"reference/cli/filters/#prometheus","title":"Prometheus","text":"<p>K8sGPT supports a Prometheus integration. Prometheus is an open source monitoring solution.</p> <p>The Prometheus integration does not deploy resources in your cluster. Instead, it detects a running Prometheus stack in the provided namespace using the <code>--namespace</code> flag. If you do not have Prometheus running, you can install it using prometheus-operator or kube-prometheus.</p> <p>Activate the Prometheus integration:</p> <pre><code>k8sgpt integration activate prometheus --namespace &lt;namespace&gt;\n</code></pre> <p>If successful, you should see the following success message displayed:</p> <pre><code>Activating prometheus integration...\nFound existing installation\nActivated integration prometheus\n</code></pre> <p>Otherwise, it will report an error:</p> <pre><code>Activating prometheus integration...\nPrometheus installation not found in namespace: &lt;namespace&gt;.\n                Please ensure Prometheus is deployed to analyze.\nError: no prometheus installation found\n</code></pre> <p>Once activated, K8sGPT will have access to new filters:</p> <pre><code>\u276f k8sgpt filters list\n\nActive:\n&gt; PersistentVolumeClaim\n&gt; Service\n&gt; ValidatingWebhookConfiguration\n&gt; MutatingWebhookConfiguration\n&gt; PrometheusConfigRelabelReport (integration)\n&gt; Deployment\n&gt; CronJob\n&gt; Node\n&gt; Pod\n&gt; PrometheusConfigValidate (integration)\n&gt; Ingress\n&gt; StatefulSet\n&gt; PrometheusConfigReport\n&gt; ReplicaSet\nUnused:\n&gt; HorizontalPodAutoScaler\n&gt; PodDisruptionBudget\n&gt; NetworkPolicy\n&gt; Log\n&gt; GatewayClass\n&gt; Gateway\n&gt; HTTPRoute\n</code></pre>"},{"location":"reference/cli/filters/#using-the-new-filters-to-analyze-your-cluster_1","title":"Using the new filters to analyze your cluster","text":"<p>Any of the filters listed in the previous section can be used as part of the <code>k8sgpt analyze</code> command.</p> <p>The <code>PrometheusConfigValidate</code> analyzer does a basic \"sanity-check\" on your Prometheus configuration to ensure it is formatted correctly and that Prometheus can load it properly. For example, if Prometheus is deployed in the <code>monitoring</code> namespace and has a bad config, we can analyze the issue using the <code>--filter</code> flag:</p> <pre><code>\u276f k8sgpt analyze --filter PrometheusConfigValidate --namespace monitoring --explain\n\n0 monitoring/prometheus-test-0(StatefulSet/prometheus-test)\n- Error: error validating Prometheus YAML configuration: unknown relabel action \"keeps\"\nError: Unknown relabel action \"keeps\" in Prometheus configuration.\n\nSolution:\n1. Check the Prometheus documentation for valid relabel actions.\n2. Correct the relabel action to a valid one, such as \"keep\" or \"drop\".\n3. Ensure the relabel configuration is correct and matches the intended behavior.\n4. Restart Prometheus to apply the changes.\n</code></pre> <p>The <code>PrometheusConfigRelabelReport</code> analyzer parses your Prometheus relabeling rules and reports groups of labels needed by your targets to be scraped successfully.</p> <pre><code>\u276f k8sgpt analyze --filter PrometheusConfigRelabelReport --namespace monitoring --explain\n\nDiscovered and parsed Prometheus scrape configurations.\nFor targets to be scraped by Prometheus, ensure they are running with\nat least one of the following label sets:\n- Job: prom-example\n  - Service Labels:\n    - app.kubernetes.io/name=prom-example\n  - Pod Labels:\n    - app.kubernetes.io/name=prom-example\n  - Namespaces:\n    - default\n  - Ports:\n    - metrics\n  - Containers:\n    - prom-example\n- Job: collector\n  - Service Labels:\n    - app.kubernetes.io/name=collector\n  - Pod Labels:\n    - app.kubernetes.io/name=collector\n  - Namespaces:\n    - monitoring\n  - Ports:\n    - prom-metrics\n  - Containers:\n    - collector\n</code></pre> <p>Note: the LLM prompt includes a subset of your Prometheus relabeling rules to avoid using too many tokens, so you may not see every label set in the output.</p>"},{"location":"reference/cli/filters/#aws","title":"AWS","text":"<p>The AWS Operator is a tool that allows Kubernetes to manage AWS resources directly, making it easier to integrate AWS services with other Kubernetes applications. This integration helps K8sGPT to interact with the AWS resources managed by the Operator. As a result, you can use K8sGPT to analyze and manage not only your Kubernetes resources but also your AWS resources that are under the management of the AWS Operator.</p> <p>Activate the AWS integration:</p> <pre><code>k8sgpt integration activate aws\n</code></pre> <p>Once activated, you should see the following success message displayed:</p> <pre><code>Activated integration aws\n</code></pre> <p>This will activate the AWS Kubernetes Operator into the Kubernetes cluster and make it possible for K8sGPT to interact with the results of the Operator.</p> <p>Once the AWS integration is activated inside the cluster, K8sGPT will have access to EKS:</p> <pre><code>\u276f k8sgpt filters list\n\nActive:\n&gt; StatefulSet\n&gt; Ingress\n&gt; Pod\n&gt; Node\n&gt; ValidatingWebhookConfiguration\n&gt; Service\n&gt; EKS (integration)\n&gt; PersistentVolumeClaim\n&gt; MutatingWebhookConfiguration\n&gt; CronJob\n&gt; Deployment\n&gt; ReplicaSet\nUnused:\n&gt; Log\n&gt; GatewayClass\n&gt; Gateway\n&gt; HTTPRoute\n&gt; HorizontalPodAutoScaler\n&gt; PodDisruptionBudget\n&gt; NetworkPolicy\n</code></pre> <p>More information can be found on the official AWS-Operator documentation.</p>"},{"location":"reference/cli/filters/#using-the-new-filters-to-analyze-your-cluster_2","title":"Using the new filters to analyze your cluster","text":"<p>Any of the filters listed in the previous section can be used as part of the <code>k8sgpt analyze</code> command.</p> <p>Note: Ensure the <code>AWS_ACCESS_KEY_ID</code> and <code>AWS_SECRET_ACCESS_KEY</code> environment variables are set as outlined in the AWS CLI environment variables documentation.</p> <p>To use the <code>EKS</code> filter from the AWS integration, specify it with the --filter flag:</p> <pre><code>k8sgpt analyze --filter EKS\n</code></pre> <p>This command analyzes your cluster's EKS resources using K8sGPT. Make sure your EKS cluster is working in the specified namespace. The report's results will vary based on the EKS reports available in your cluster.</p>"},{"location":"reference/cli/filters/#kyverno","title":"Kyverno","text":"<p>Kyverno is a policy engine designed for Kubernetes.</p> <p>Kyverno must be installed prior to using this integration.</p> <p>To activate the Kyverno integration:</p> <pre><code>k8sgpt integration activate kyverno\n\nk8sgpt integration list\nActive:\n&gt; kyverno\nUnused: \n&gt; trivy\n&gt; prometheus\n&gt; aws\n&gt; keda\n</code></pre> <p>The following filters will become available:</p> <ul> <li>PolicyReport</li> <li>ClusterPolicyReport</li> </ul> <pre><code>k8sgpt filters list    \nActive: \n&gt; ClusterPolicyReport (integration)\n&gt; ReplicaSet\n&gt; Service\n&gt; StatefulSet\n&gt; PersistentVolumeClaim\n&gt; ValidatingWebhookConfiguration\n&gt; MutatingWebhookConfiguration\n&gt; PolicyReport (integration)\n&gt; Node\n&gt; Pod\n&gt; Deployment\n&gt; Ingress\n&gt; CronJob\nUnused: \n&gt; Log\n&gt; GatewayClass\n&gt; Gateway\n&gt; HTTPRoute\n&gt; HorizontalPodAutoScaler\n&gt; PodDisruptionBudget\n&gt; NetworkPolicy\n</code></pre> <p>Policy reports are generated and managed by Kyverno. You can learn more about this here https://kyverno.io/docs/policy-reports/.</p> <p>Kyverno is currently only supported via the CLI, an operator is being developed.</p>"},{"location":"reference/cli/filters/#adding-and-removing-default-filters","title":"Adding and removing default filters","text":"<p>Remove default filters</p> <pre><code>k8sgpt filters add [filter(s)]\n</code></pre> <ul> <li>Single filter : <code>k8sgpt filters add Service</code></li> <li>Multiple filters : <code>k8sgpt filters add Ingress,Pod</code></li> </ul> <p>Remove default filters</p> <pre><code>k8sgpt filters remove [filter(s)]\n</code></pre> <ul> <li>Simple filter : <code>k8sgpt filters remove Service</code></li> <li>Multiple filters : <code>k8sgpt filters remove Ingress,Pod</code></li> </ul>"},{"location":"reference/cli/serve-mode/","title":"K8sGPT Serve mode","text":""},{"location":"reference/cli/serve-mode/#prerequisites","title":"Prerequisites","text":"<ol> <li>Have grpcurl installed</li> </ol>"},{"location":"reference/cli/serve-mode/#run-k8sgpt-serve-mode","title":"Run <code>k8sgpt</code> serve mode","text":"<pre><code>$ k8sgpt serve\n{\"level\":\"info\",\"ts\":1684309627.113916,\"caller\":\"server/server.go:83\",\"msg\":\"binding metrics to 8081\"}\n{\"level\":\"info\",\"ts\":1684309627.114198,\"caller\":\"server/server.go:68\",\"msg\":\"binding api to 8080\"}\n</code></pre> <p>This command starts two servers:</p> <ol> <li>The health server runs on port 8081 by default and serves metrics and health endpoints.</li> <li>The API server runs on port 8080 (gRPC) by default and serves the analysis handler.</li> </ol> <p>For more details about the gRPC implementation, refer to this link.</p>"},{"location":"reference/cli/serve-mode/#analyze-your-cluster-with-grpcurl","title":"Analyze your cluster with <code>grpcurl</code>","text":"<p>Make sure you are connected to a Kubernetes cluster:</p> <pre><code>kubectl get nodes\n</code></pre> <p>Next, run the following command:</p> <pre><code>grpcurl -plaintext localhost:8080 schema.v1.ServerService/Analyze\n</code></pre> <p>This command provides a list of issues in your Kubernetes cluster. If there are no issues identified, you should receive a status of <code>OK</code>.</p>"},{"location":"reference/cli/serve-mode/#analyze-with-parameters","title":"Analyze with parameters","text":"<p>You can specify parameters using the following command:</p> <pre><code>grpcurl -plaintext -d '{\"explain\": false, \"filters\": [\"Ingress\"], \"namespace\": \"k8sgpt\"}' localhost:8080 schema.v1.ServerService/Analyze\n</code></pre> <p>In this example, the analyzer will only consider the <code>k8sgpt</code> namespace without AI explanation and only focus on the <code>Ingress</code> filter.</p>"},{"location":"reference/guidelines/community/","title":"Community Information","text":"<p>All community related information are kept in a separate repository from the docs.</p> <p>Link to the repository: k8sgpt-ai/community</p> <p>There you will find information on</p> <ul> <li>The Charter</li> <li>Adopters List</li> <li>Code of Conduct</li> <li>Community Members</li> <li>Subprojects</li> </ul> <p>and much more.</p>"},{"location":"reference/guidelines/guidelines/","title":"Contributing Guidelines","text":""},{"location":"reference/guidelines/guidelines/#contributing-to-the-documentation","title":"Contributing to the Documentation","text":"<p>This documentation follows the Diataxis framework. If you are proposing completely new content to the documentation, please familiarise yourself with the framework first.</p> <p>The documentation is created with mkdocs, specifically the Material for MkDocs theme.</p>"},{"location":"reference/guidelines/guidelines/#contributing-projects-in-the-k8sgpt-organisation","title":"Contributing projects in the K8sGPT organisation","text":"<p>All projects in the K8sGPT organisation follow our contributing guidelines.</p>"},{"location":"reference/guidelines/privacy/","title":"Privacy","text":"<p>K8sGPT is a privacy-first tool and believe transparency is key for you to understand how we use your data. We have created this page to help you understand how we collect, use, share and protect your data.</p>"},{"location":"reference/guidelines/privacy/#data-we-collect","title":"Data we collect","text":"<p>K8sGPT will collect data from Analyzers and either display it directly to you or with the <code>--explain</code> flag it will send it to the selected AI backend.</p> <p>The type of data collected depends on the Analyzer you are using. For example, the <code>k8sgpt analyze pod</code> command will collect the following data: - Container status message - Pod name - Pod namespace - Event message</p>"},{"location":"reference/guidelines/privacy/#data-we-share","title":"Data we share","text":"<p>As mentioned, K8sGPT will share data with the selected AI backend only when you choose <code>--explain</code> and <code>auth</code> against that backend. The data shared will be the same as the data collected.</p> <p>To learn more about the privacy policy of our default AI backend OpenAI please visit their privacy policy.</p>"},{"location":"reference/guidelines/privacy/#data-we-protect","title":"Data we protect","text":"<p>When you are sending data through the <code>--explain</code> option, there is the capability of anonymising some of that data. This is done by using the <code>--anonymize</code> flag. In the example of the Deployment Analyzer, this will obfuscate the following data:</p> <ul> <li>Deployment name</li> <li>Deployment namespace</li> </ul>"},{"location":"reference/guidelines/privacy/#data-we-dont-collect","title":"Data we don't collect","text":"<ul> <li>Logs</li> <li>API Server data other than the primitives used within our Analyzers.</li> </ul>"},{"location":"reference/guidelines/privacy/#contact","title":"Contact","text":"<p>If you have any questions about our privacy policy, please contact us.</p>"},{"location":"reference/operator/advanced-installation/","title":"Advanced Operator installation options","text":"<p>This documentation lists advanced installation options for the K8sGPT Operator.</p>"},{"location":"reference/operator/advanced-installation/#argocd","title":"ArgoCD","text":"<p>ArgoCD is a continuous deployment tool that implements GitOps best practices to install and manage Kubernetes resources.</p>"},{"location":"reference/operator/advanced-installation/#prerequisites","title":"Prerequisites","text":"<p>To install and manage K8sGPT through ArgoCD, ensure that you have ArgoCD installed and running inside your cluster. The ArgoCD getting-started-guide provides detailed information.</p>"},{"location":"reference/operator/advanced-installation/#installing-k8sgpt","title":"Installing K8sGPT","text":"<p>K8sGPT can be installed through ArgoCD by applying an <code>Application</code> CRD to the ArgoCD namespaces in your cluster (with ArgoCD running):</p> <p>K8sGPT Application CRD:</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: k8sgpt\n  namespace: argocd\nspec:\n  project: default\n  source:\n    chart: k8sgpt-operator\n    repoURL: https://charts.k8sgpt.ai/\n    targetRevision: &lt;VERSION&gt;\n    helm:\n      values: |\n        serviceMonitor:\n          enabled: true\n        GrafanaDashboard:\n          enabled: true\n  destination:\n    server: https://kubernetes.default.svc\n    namespace: k8sgpt-operator-system\n  syncPolicy:\n    automated:\n      prune: true\n      selfHeal: true\n    syncOptions:\n      - CreateNamespace=true\n</code></pre> <p>Note:</p> <ul> <li>Ensure that the <code>namespace</code> is correctly set to your ArgoCD namespace.</li> <li>Ensure that the <code>&lt;VERSION&gt;</code> is set to the K8sGPT Operator Release Version that you want to use.</li> <li>Modify the <code>helm.values</code> section with the Helm Values that you would like to overwrite. Check the values.yaml file of the Operator for options.</li> </ul> <p>Applying the resource:</p> <pre><code>kubectl apply -f application.yaml\n</code></pre>"},{"location":"reference/operator/advanced-installation/#installing-the-remaining-operator-resources","title":"Installing the remaining Operator resources","text":"<p>You will still need to install the</p> <ul> <li>K8sGPT Operator CRD</li> <li>K8sGPT secret to access the AI backend</li> </ul> <p>that are both detailed in the Operator installation page. The above application resource will only install the Operator pods themselves not additional resources. Note that you could manage those resources also through ArgoCD. Please refer to the official ArgoCD documentation for further information.</p>"},{"location":"reference/operator/overview/","title":"K8sGPT Operator","text":"<p>K8sGPT can run as a Kubernetes Operator inside the cluster. The scan results are provided as Kubernetes YAML manifests.</p> <p>This section will only detail how to configure the operator. For installation instructions, please see the getting-started section.</p>"},{"location":"reference/operator/overview/#architecture","title":"Architecture","text":"<p>The diagram below showcases the different components that the K8sGPT Operator installs and manages:</p> <p></p>"},{"location":"reference/operator/overview/#customising-the-operator","title":"Customising the Operator","text":"<p>As with other Helm Charts, the K8sGPT Operator can be customised by modifying  the  <code>values.yaml</code> file.</p> <p>The following fields can be customised in the Helm Chart Deployment:</p> Parameter Description Default <code>serviceMonitor.enabled</code> <code>false</code> <code>serviceMonitor.additionalLabels</code> <code>{}</code> <code>grafanaDashboard.enabled</code> <code>false</code> <code>grafanaDashboard.folder.annotation</code> <code>\"grafana_folder\"</code> <code>grafanaDashboard.folder.name</code> <code>\"ai\"</code> <code>grafanaDashboard.label.key</code> <code>\"grafana_dashboard\"</code> <code>grafanaDashboard.label.value</code> <code>\"1\"</code> <code>controllerManager.kubeRbacProxy.containerSecurityContext.allowPrivilegeEscalation</code> <code>false</code> <code>controllerManager.kubeRbacProxy.containerSecurityContext.capabilities.drop</code> <code>[\"ALL\"]</code> <code>controllerManager.kubeRbacProxy.image.repository</code> <code>\"gcr.io/kubebuilder/kube-rbac-proxy\"</code> <code>controllerManager.kubeRbacProxy.image.tag</code> <code>\"v0.0.19\"</code> <code>controllerManager.kubeRbacProxy.resources.limits.cpu</code> <code>\"500m\"</code> <code>controllerManager.kubeRbacProxy.resources.limits.memory</code> <code>\"128Mi\"</code> <code>controllerManager.kubeRbacProxy.resources.requests.cpu</code> <code>\"5m\"</code> <code>controllerManager.kubeRbacProxy.resources.requests.memory</code> <code>\"64Mi\"</code> <code>controllerManager.manager.sinkWebhookTimeout</code> <code>\"30s\"</code> <code>controllerManager.manager.containerSecurityContext.allowPrivilegeEscalation</code> <code>false</code> <code>controllerManager.manager.containerSecurityContext.capabilities.drop</code> <code>[\"ALL\"]</code> <code>controllerManager.manager.image.repository</code> <code>\"ghcr.io/k8sgpt-ai/k8sgpt-operator\"</code> <code>controllerManager.manager.image.tag</code> <code>\"v0.0.19\"</code> <code>controllerManager.manager.resources.limits.cpu</code> <code>\"500m\"</code> <code>controllerManager.manager.resources.limits.memory</code> <code>\"128Mi\"</code> <code>controllerManager.manager.resources.requests.cpu</code> <code>\"10m\"</code> <code>controllerManager.manager.resources.requests.memory</code> <code>\"64Mi\"</code> <code>controllerManager.replicas</code> <code>1</code> <code>kubernetesClusterDomain</code> <code>\"cluster.local\"</code> <code>metricsService.ports</code> <code>[{\"name\": \"https\", \"port\": 8443, \"protocol\": \"TCP\", \"targetPort\": \"https\"}]</code> <code>metricsService.type</code> <code>\"ClusterIP\"</code>"},{"location":"reference/operator/overview/#for-example-in-cluster-metrics","title":"For example: In-cluster metrics","text":"<p>It is possible to enable metrics of the operator so that they can be scraped through Prometheus.</p> <p>This is the configuration required in the <code>values.yaml</code> manifest:</p> <pre><code>serviceMonitor:\n  enabled: true\n</code></pre> <p>The new <code>values.yaml</code> manifest can then be provided upon installing the Operator inside the cluster:</p> <pre><code>helm update --install release k8sgpt/k8sgpt-operator --values values.yaml\n</code></pre>"},{"location":"reference/providers/backend/","title":"K8sGPT AI Backends","text":"<p>A Backend (also called Provider) is a service that provides access to the AI language model. There are many different backends available for K8sGPT. Each backend has its own strengths and weaknesses, so it is important to choose the one that is right for your needs.</p> <p>Currently, we have a total of 11 backends available:</p> <ul> <li>OpenAI</li> <li>Cohere</li> <li>Amazon Bedrock</li> <li>Amazon SageMaker</li> <li>Azure OpenAI</li> <li>Google Gemini</li> <li>Google Vertex AI</li> <li>Hugging Face</li> <li>IBM watsonx.ai</li> <li>LocalAI</li> <li>Ollama</li> <li>FakeAI</li> </ul>"},{"location":"reference/providers/backend/#openai","title":"OpenAI","text":"<p>OpenAI is the default backend for K8sGPT. We recommend using OpenAI first if you are new to K8sGPT and if you have an account on OpenAI. OpenAI comes with the access to powerful language models such as GPT-4. If you are looking for a powerful and easy-to-use language modeling service, OpenAI is a great option.</p> <ul> <li>To use OpenAI you'll need an OpenAI token for authentication purposes. To generate a token use:     <code>bash     k8sgpt generate</code></li> <li>To set the token in K8sGPT, use the following command:     <code>bash     k8sgpt auth add</code></li> <li>Run the following command to analyze issues within your cluster using OpenAI:     <code>bash     k8sgpt analyze --explain</code></li> </ul>"},{"location":"reference/providers/backend/#cohere","title":"Cohere","text":"<p>Cohere allows building conversational apps. It uses Retrieval Augmented Generation (RAG) toolkit that improves LLM's answer accuracy.</p> <ul> <li>To use Cohere, visit Cohere dashboard.</li> <li>To configure backend in K8sGPT, use the following command:     <code>bash     k8sgpt auth add --backend cohere --model command-nightly</code></li> <li>Run the following command to analyze issues within your cluster using Cohere:     <code>bash     k8sgpt analyze --explain --backend cohere</code></li> </ul>"},{"location":"reference/providers/backend/#amazon-bedrock","title":"Amazon Bedrock","text":"<p>Amazon Bedrock allows building and scaling generative AI applications.</p> <ul> <li>To use Bedrock, make sure you have access to Bedrock API and models e.g. in AWS Console you should see something like this:</li> </ul> <p></p> <ul> <li> <p>You will need to set the follow local environmental variables:     ```</p> <ul> <li>AWS_ACCESS_KEY</li> <li>AWS_SECRET_ACCESS_KEY</li> <li>AWS_DEFAULT_REGION ```</li> </ul> </li> <li> <p>To configure backend in K8sGPT use auth command:     <code>bash     k8sgpt auth add --backend amazonbedrock --model anthropic.claude-v2</code></p> </li> <li>Run the following command to analyze issues within your cluster using Amazon Bedrock:     <code>bash     k8sgpt analyze --explain --backend amazonbedrock</code></li> </ul>"},{"location":"reference/providers/backend/#amazon-sagemaker","title":"Amazon SageMaker","text":"<p>The Amazon SageMaker backend allows you to leverage a self-deployed and managed Language Models (LLM) on Amazon SageMaker.</p> <p>Example how to deploy Amazon SageMaker with cdk is available in llm-sagemaker-jumpstart-cdk repo.</p> <ul> <li>To use SageMaker, make sure you have the AWS CLI configured on your machine.</li> <li>You need to have an Amazon SageMaker instance set up.</li> <li>Run the following command to add SageMaker:     <code>bash     k8sgpt auth add --backend amazonsagemaker --providerRegion eu-west-1 --endpointname endpoint-xxxxxxxxxx</code></li> <li>Now you are ready to analyze with the Amazon SageMaker backend:     <code>bash     k8sgpt analyze --explain --backend amazonsagemaker</code></li> </ul>"},{"location":"reference/providers/backend/#azure-openai","title":"Azure OpenAI","text":"<p>Azure OpenAI Provider provides REST API access to OpenAI's powerful language models. It gives the users an advanced language AI with powerful models with the security and enterprise promise of Azure.</p> <ul> <li> <p>The Azure OpenAI Provider requires a deployment as a prerequisite. You can visit their documentation to create your own.   To authenticate with k8sgpt, you would require an Azure OpenAI endpoint of your tenant <code>https://your Azure OpenAI Endpoint</code>,the API key to access your deployment, the deployment name of your model and the model name itself.</p> </li> <li> <p>Run the following command to authenticate with Azure OpenAI:     <code>bash     k8sgpt auth add --backend azureopenai --baseurl https://&lt;your Azure OpenAI endpoint&gt; --engine &lt;deployment_name&gt; --model &lt;model_name&gt;</code></p> </li> <li>Now you are ready to analyze with the Azure OpenAI backend:     <code>bash     k8sgpt analyze --explain --backend azureopenai</code></li> </ul>"},{"location":"reference/providers/backend/#google-gemini","title":"Google Gemini","text":"<p>Google Gemini allows generative AI capabilities with multimodal approach (it is capable to understand not only text, but also code, audio, image and video). With Gemini models, a new API was introduced, and this is what is now built-in K8sGPT. This API also works against the Google Cloud Vertex AI service. See also Google AI Studio to get started.</p> <p>NOTE: Gemini API might be still rolling to some regions. See the available regions for details.</p> <ul> <li>To use Google Gemini API in K8sGPT, obtain the API key.</li> <li>To configure Google backend in K8sGPT with <code>gemini-pro</code> model (see all models here) use auth command:     <code>bash     k8sgpt auth add --backend googlevertexai --model gemini-pro --password \"&lt;Your API KEY&gt;\"</code></li> <li>Run the following command to analyze issues within your cluster with the Google provider:     <code>bash     k8sgpt analyze --explain --backend google</code></li> </ul>"},{"location":"reference/providers/backend/#google-gemini-via-vertex-ai","title":"Google Gemini via Vertex AI","text":"<p>Google Gemini allows generative AI capabilities with multimodal approach (it is capable to understand not only text, but also code, audio, image and video). </p> <ul> <li>To use Google Vertex AI you need to be authorized via Google Cloud SDK.      The Vertex AI API needs to be enabled.</li> </ul> <p>Note: Vertex AI Gemini API is currently available in these regions, verify if those are working for your environment</p> <ul> <li> <p>Open a terminal or command prompt and run the following command to authenticate using your Google Cloud credentials:     <code>bash     gcloud auth application-default login</code></p> </li> <li> <p>To configure Google backend in K8sGPT with <code>gemini-pro</code> model (see all models here) use auth command:     <code>bash     k8sgpt auth add --backend googlevertexai --model \"gemini-pro\" --providerRegion \"us-central1\" --providerId \"&lt;your project id&gt;\"</code></p> </li> <li>Run the following command to analyze issues within your cluster with the Google provider:     <code>bash     k8sgpt analyze --explain --backend googlevertexai</code> </li> </ul>"},{"location":"reference/providers/backend/#huggingface","title":"HuggingFace","text":"<p>Hugging Face is a versatile backend for K8sGPT, offering access to a wide range of pre-trained language models. It provides easy-to-use interfaces for both training and inference tasks. Refer to the Hugging Face documentation for further insights into model usage and capabilities.</p> <ul> <li>To use Hugging Face API in K8sGPT, obtain the API key.</li> <li> <p>Configure the HuggingFace backend in K8sGPT by specifying the desired model (see all models here) using auth command:     <code>bash     k8sgpt auth add --backend huggingface --model &lt;model name&gt;</code></p> <p>NOTE: Since the default gpt-4o-mini model is not available in Hugging Face, a valid backend model is required.</p> </li> <li> <p>Once configured, you can analyze issues within your cluster using the Hugging Face provider with the following command:     <code>bash     k8sgpt analyze --explain --backend huggingface</code></p> </li> </ul>"},{"location":"reference/providers/backend/#ibm-watsonxai","title":"IBM watsonx.ai","text":"<p>IBM\u00ae watsonx.ai\u2122 AI studio is part of the IBM watsonx\u2122 AI and data platform, bringing together new generative AI (gen AI) capabilities powered by foundation models and traditional machine learning (ML) into a powerful studio spanning the AI lifecycle. Tune and guide models with your enterprise data to meet your needs with easy-to-use tools for building and refining performant prompts. With watsonx.ai, you can build AI applications in a fraction of the time and with a fraction of the data. </p> <ul> <li> <p>To use IBM watsonx.ai, you'll need a watsonx API key and project ID for authentication.</p> </li> <li> <p>You will need to set the follow local environmental variables:     ```</p> <ul> <li>WATSONX_API_KEY</li> <li>WATSONX_PROJECT_ID ```</li> </ul> </li> <li>To configure backend in K8sGPT use auth command:     <code>bash     k8sgpt auth add --backend watsonxai --model ibm/granite-13b-chat-v2</code></li> <li>Run the following command to analyze issues within your cluster using IBM watsonx.ai:     <code>bash     k8sgpt analyze --explain --backend watsonxai</code></li> </ul>"},{"location":"reference/providers/backend/#localai","title":"LocalAI","text":"<p>LocalAI is a local model, which is an OpenAI compatible API. It uses llama.cpp and ggml to run inference on consumer-grade hardware. Models supported by LocalAI for instance are Vicuna, Alpaca, LLaMA, Cerebras, GPT4ALL, GPT4ALL-J and koala.</p> <ul> <li>To run local inference, you need to download the models first, for instance you can find <code>ggml</code> compatible models in huggingface.com(for example vicuna, alpaca and koala).</li> <li>To start the API server, follow the instruction in LocalAI.</li> <li>Authenticate K8sGPT with LocalAI:     <code>bash     k8sgpt auth add --backend localai --model &lt;model_name&gt; --baseurl http://localhost:8080/v1</code></li> <li>Analyze with a LocalAI backend:     <code>bash     k8sgpt analyze --explain --backend localai</code></li> </ul>"},{"location":"reference/providers/backend/#ollama-via-localai-backend","title":"Ollama (via LocalAI backend)","text":"<p>Ollama is a local model, which has an OpenAI compatible API. It supports the models listed in the Ollama library. </p> <ul> <li>To start the API server, follow the instruction in the Ollama docs.</li> <li>Authenticate K8sGPT with LocalAI:     <code>bash     k8sgpt auth add --backend localai --model &lt;model_name&gt; --baseurl http://localhost:11434</code></li> <li>Analyze with a LocalAI backend:     <code>bash     k8sgpt analyze --explain --backend localai</code></li> </ul>"},{"location":"reference/providers/backend/#ollama","title":"Ollama","text":"<p>Ollama can get up and running locally with large language models. It runs Llama 2, Code Llama, and other models.</p> <ul> <li> <p>To start the Ollama server, follow the instruction in Ollama.     <code>bash     ollama serve</code>   It can also run as an docker image, follow the instruction in Ollama BLog <code>bash     docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama</code></p> </li> <li> <p>Authenticate K8sGPT with Ollama:     <code>bash     k8sgpt auth add --backend ollama --model llama2 --baseurl http://localhost:11434</code></p> </li> <li>Analyze with a Ollama backend:     <code>bash     k8sgpt analyze --explain --backend ollama</code></li> </ul>"},{"location":"reference/providers/backend/#fakeai","title":"FakeAI","text":"<p>FakeAI or the NoOpAiProvider might be useful in situations where you need to test a new feature or simulate the behaviour of an AI based-system without actually invoking it. It can help you with local development, testing and troubleshooting. The NoOpAiProvider does not actually perform any AI-based operations but simulates them by echoing the input given as a problem.</p> <p>Follow the steps outlined below to learn how to utilize the NoOpAiProvider:</p> <ul> <li>Authorize k8sgpt with <code>noopai</code> or <code>noop</code> as the Backend Provider:     <code>k8sgpt auth add -b noopai</code></li> <li> <p>For the auth token, you can leave it blank as the NoOpAiProvider is configured to work fine with or without any token.</p> </li> <li> <p>Use the analyze and explain command to check for errors in your kubernetes cluster and the NoOpAiProvider should return the error as the solution itself:     <code>k8sgpt analyze --explain --backend noopai</code></p> </li> </ul>"},{"location":"tutorials/","title":"Tutorials","text":"<p>This section provides:</p> <ul> <li>end-to-end tutorials on specific use cases</li> <li>a collection of user and contributor created content</li> </ul>"},{"location":"tutorials/custom-analyzers/","title":"Custom Analyzers","text":"<p>In this tutorial, we will learn how to create custom analyzers for K8sGPT. We will create a custom analyzer that checks a Linux host for resource issues and provides recommendations.</p> <p>Full example code</p>"},{"location":"tutorials/custom-analyzers/#why","title":"Why?","text":"<p>There are usecases where you might want to create custom analyzers to check for specific issues in your environment. This would be in conjunction with the K8sGPT built-in analyzers. For example, you may wish to scan the Kubernetes cluster nodes more deeply to understand if there are underlying issues that are related to issues in the cluster.</p>"},{"location":"tutorials/custom-analyzers/#prerequisites","title":"Prerequisites","text":"<ul> <li>K8sGPT CLI</li> <li>Golang go1.22 or higher</li> </ul>"},{"location":"tutorials/custom-analyzers/#writing-a-simple-analyzer","title":"Writing a simple analyzer","text":"<p>The K8sGPT CLI, operator and custom analyzers all use a GRPC API to communicate with each other. The API is defined in the buf.build/k8sgpt-ai/k8sgpt repository. Buf is a tool that helps you manage Protobuf files. You can install it by following the instructions here.  Another advantage of buf is that when you import a Protobuf file, it will automatically download the dependencies for you. This is useful when you are working with Protobuf files that have dependencies on other Protobuf files. Additionally, you'll always be able to get the latest version of the Protobuf files.</p>"},{"location":"tutorials/custom-analyzers/#project-setup","title":"Project setup","text":"<p>Let's create a new simple golang project. We will use the following directory structure:</p> <pre><code>mkdir -p custom-analyzer\ncd custom-analyzer\ngo mod init github.com/&lt;username&gt;/custom-analyzer\n</code></pre> <p>Once we have this structure let's create a simple main.go file with the following content:</p> <pre><code>// main.go\npackage main\n\nimport (\n    \"errors\"\n    \"fmt\"\n    \"net\"\n    \"net/http\"\n\n    rpc \"buf.build/gen/go/k8sgpt-ai/k8sgpt/grpc/go/schema/v1/schemav1grpc\"\n    \"github.com/k8sgpt-ai/go-custom-analyzer/pkg/analyzer\"\n    \"google.golang.org/grpc\"\n    \"google.golang.org/grpc/reflection\"\n)\n\nfunc main() {\n    fmt.Println(\"Starting!\")\n    var err error\n    address := fmt.Sprintf(\":%s\", \"8085\")\n    lis, err := net.Listen(\"tcp\", address)\n    if err != nil {\n        panic(err)\n    }\n    grpcServer := grpc.NewServer()\n    reflection.Register(grpcServer)\n    aa := analyzer.Analyzer{}\n    rpc.RegisterCustomAnalyzerServiceServer(grpcServer, aa.Handler)\n    if err := grpcServer.Serve(\n        lis,\n    ); err != nil &amp;&amp; !errors.Is(err, http.ErrServerClosed) {\n        return\n    }\n}\n</code></pre> <p>The most important part of this file is here:</p> <pre><code>aa := analyzer.Analyzer{}\n    rpc.RegisterAnalyzerServiceServer(grpcServer, aa.Handler)\n</code></pre> <p>Let's go ahead and create the <code>analyzer</code> package with the following structure:</p> <pre><code>mkdir -p pkg/analyzer\n</code></pre> <p>Now let's create the <code>analyzer.go</code> file with the following content:</p> <pre><code>// analyzer.go\n\npackage analyzer\n\nimport (\n    \"context\"\n    \"fmt\"\n\n    rpc \"buf.build/gen/go/k8sgpt-ai/k8sgpt/grpc/go/schema/v1/schemav1grpc\"\n    v1 \"buf.build/gen/go/k8sgpt-ai/k8sgpt/protocolbuffers/go/schema/v1\"\n    \"github.com/ricochet2200/go-disk-usage/du\"\n)\n\ntype Handler struct {\n    rpc.CustomAnalyzerServiceServer\n}\ntype Analyzer struct {\n    Handler *Handler\n}\n\nfunc (a *Handler) Run(context.Context, *v1.RunRequest) (*v1.RunResponse, error) {\n    response := &amp;v1.RunResponse{\n        Result: &amp;v1.Result{\n            Name:    \"example\",\n            Details: \"example\",\n            Error: []*v1.ErrorDetail{\n                &amp;v1.ErrorDetail{\n                    Text: \"This is an example error message!\",\n                },\n            },\n        },\n    }\n\n    return response, nil\n}\n</code></pre> <p>This file contains the <code>Handler</code> struct which implements the <code>Run</code> method. This method is called when the analyzer is run. In this example, we are returning an error message. The <code>Run</code> method takes a context and an <code>RunRequest</code> as arguments and returns an <code>RunResponse</code> and an error. Find the API available here.</p>"},{"location":"tutorials/custom-analyzers/#implementing-some-custom-logic","title":"Implementing some custom logic","text":"<p>Now that we have the basic structure in place, let's implement some custom logic. We will check the disk usage on the host and return an error if it is above a certain threshold.</p> <pre><code>// analyzer.go\nfunc (a *Handler) Run(context.Context, *v1.RunRequest) (*v1.RunResponse, error) {\n    println(\"Running analyzer\")\n    usage := du.NewDiskUsage(\"/\")\n    diskUsage := int((usage.Size() - usage.Free()) * 100 / usage.Size())\n    return &amp;v1.RunResponse{\n        Result: &amp;v1.Result{\n            Name:    \"diskuse\",\n            Details: fmt.Sprintf(\"Disk usage is %d\", diskUsage),\n            Error: []*v1.ErrorDetail{\n                {\n                    Text: fmt.Sprintf(\"Disk usage is %d\", diskUsage),\n                },\n            },\n        },\n    }, nil\n}\n</code></pre>"},{"location":"tutorials/custom-analyzers/#testing-it-out","title":"Testing it out","text":"<p>To test this with K8sGPT we need to update the local K8sGPT CLI configuration to point to the custom analyzer. We can do this by running the following command:</p> <pre><code>k8sgpt custom-analyzer add -n diskuse\n</code></pre> <p>This will add the custom analyzer <code>diskuse</code> to the list of available analyzers in the K8sGPT CLI.</p> <pre><code>k8sgpt custom-analyzer list\nActive:\n&gt; diskuse\n</code></pre> <p>To execute the analyzer we can run the following command:</p> <ul> <li>run the customer analyzer</li> </ul> <pre><code>go run main.go\n</code></pre> <ul> <li>execute the analyzer</li> </ul> <pre><code>k8sgpt analyze --custom-analysis\n</code></pre>"},{"location":"tutorials/custom-analyzers/#whats-next","title":"What's next?","text":"<p>Now you've got the basics of how to write a custom analyzer, you can extend this to check for other issues on your hosts or in your Kubernetes cluster. You can also create more complex analyzers that check for multiple issues and provide more detailed recommendations.</p>"},{"location":"tutorials/custom-rest-backend/","title":"Custom Rest Backend","text":"<p>This tutorial guides you through the process of integrating a custom backend with k8sgpt using RESTful API. This setup is particularly useful when you want to integrate Retrieval-Augmented Generation (RAG) or an AI Agent with k8sgpt. In this tutorial, we will store a CNCF Q&amp;A dataset for knowledge retrieval and  create a simple Retrieval-Augmented Generation (RAG) application and integrate it with k8sgpt. </p>"},{"location":"tutorials/custom-rest-backend/#api-specification","title":"API Specification","text":"<p>To ensure k8sgpt can interact with your custom backend, implement the following API endpoint using the OpenAPI schema:</p>"},{"location":"tutorials/custom-rest-backend/#openapi-specification","title":"OpenAPI Specification","text":"<pre><code>openapi: 3.0.0\ninfo:\n  title: Custom REST Backend API\n  version: 1.0.0\npaths:\n  /v1/completions:\n    post:\n      summary: Generate a text-based response from the custom backend\n      requestBody:\n        required: true\n        content:\n          application/json:\n            schema:\n              type: object\n              properties:\n                model:\n                  type: string\n                  description: The name of the model to use.\n                prompt:\n                  type: string\n                  description: The textual prompt to send to the model.\n                options:\n                  type: object\n                  additionalProperties:\n                    type: string\n                  description: Model-specific options, such as temperature.\n              required:\n                - model\n                - prompt\n      responses:\n        \"200\":\n          description: Successful response\n          content:\n            application/json:\n              schema:\n                type: object\n                properties:\n                  model:\n                    type: string\n                    description: The model name that generated the response.\n                  created_at:\n                    type: string\n                    format: date-time\n                    description: The timestamp of the response.\n                  response:\n                    type: string\n                    description: The textual response itself.\n                required:\n                  - model\n                  - created_at\n                  - response\n        \"400\":\n          description: Bad Request\n        \"500\":\n          description: Internal Server Error\n</code></pre>"},{"location":"tutorials/custom-rest-backend/#example-interaction","title":"Example Interaction","text":""},{"location":"tutorials/custom-rest-backend/#request","title":"Request","text":"<pre><code>{\n  \"model\": \"gpt-4\",\n  \"prompt\": \"Explain the process of photosynthesis.\",\n  \"options\": {\n    \"temperature\": 0.7,\n    \"max_tokens\": 150\n  }\n}\n</code></pre>"},{"location":"tutorials/custom-rest-backend/#response","title":"Response","text":"<pre><code>{\n  \"model\": \"gpt-4\",\n  \"created_at\": \"2025-01-14T10:00:00Z\",\n  \"response\": \"Photosynthesis is the process by which green plants and some other organisms use sunlight to synthesize foods with the help of chlorophyll.\"\n}\n</code></pre>"},{"location":"tutorials/custom-rest-backend/#implementation-notes","title":"Implementation Notes","text":"<ul> <li> <p>Endpoint Configuration: Ensure the /v1/completions endpoint is reachable and adheres to the provided schema.</p> </li> <li> <p>Error Handling: Implement robust error handling to manage invalid requests or processing failures.</p> </li> </ul> <p>By following this specification, your custom REST service will seamlessly integrate with k8sgpt, enabling powerful and customizable AI-driven functionalities.</p>"},{"location":"tutorials/custom-rest-backend/#prerequisites","title":"Prerequisites","text":"<ul> <li>K8sGPT CLI</li> <li>Golang go1.22 or higher</li> <li>langchaingo library for building RAG applications</li> <li>gin for handling RESTful APIs in Go</li> <li>Qdrant vector database for storing and searching through knowledge bases</li> <li>Ollama service to run large language models</li> </ul>"},{"location":"tutorials/custom-rest-backend/#writing-a-simple-rag-backend","title":"Writing a simple RAG backend","text":""},{"location":"tutorials/custom-rest-backend/#setup","title":"Setup","text":"<p>Let's create a new simple golang project.</p> <pre><code>mkdir -p custom-backend\ncd custom-backend\ngo mod init github.com/&lt;username&gt;/custom-backend\n</code></pre> <p>Install necessary dependencies for the RAG application and RESTful API:</p> <pre><code>go get -u github.com/tmc/langchaingo\ngo get -u github.com/gin-gonic/gin\n</code></pre> <p>Once we have this structure let's create a simple main.go file with the following content:</p> <pre><code>// main.go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"net/http\"\n    \"net/url\"\n    \"strings\"\n    \"time\"\n\n    \"github.com/gin-gonic/gin\"\n    \"github.com/tmc/langchaingo/embeddings\"\n    \"github.com/tmc/langchaingo/llms\"\n    \"github.com/tmc/langchaingo/llms/ollama\"\n    \"github.com/tmc/langchaingo/vectorstores\"\n    \"github.com/tmc/langchaingo/vectorstores/qdrant\"\n)\n\nvar (\n    ollama_url = \"http://localhost:11434\"\n    listenAddr = \":8090\"\n)\n\nfunc main() {\n    server := gin.Default()\n    server.POST(\"/completion\", func(c *gin.Context) {\n        var req CustomRestRequest\n        if err := c.ShouldBindJSON(&amp;req); err != nil {\n            c.JSON(http.StatusBadRequest, gin.H{\"error\": err.Error()})\n            return\n        }\n        content, err := rag(ollama_url, req)\n        if err != nil {\n            c.JSON(http.StatusInternalServerError, gin.H{\"error\": err.Error()})\n            return\n        }\n        resp := CustomRestResponse{\n            Model:     req.Model,\n            CreatedAt: time.Now(),\n            Response:  content,\n        }\n        c.JSON(http.StatusOK, resp)\n    })\n    // start backend server\n    err := server.Run(listenAddr)\n    if err != nil {\n        fmt.Println(\"Error: %w\", err)\n    }\n}\n</code></pre> <p>This basic implementation sets up a RESTful API endpoint <code>/completion</code> that receives a <code>CustomRestRequest</code> from k8sgpt and return <code>CustomRestResponse</code>. The <code>rag</code> function handles the RAG logic. The structure of request and response is as follows:</p> <pre><code>type CustomRestRequest struct {\n    Model string `json:\"model\"`\n\n    // Prompt is the textual prompt to send to the model.\n    Prompt string `json:\"prompt\"`\n\n    // Options lists model-specific options. For example, temperature can be\n    // set through this field, if the model supports it.\n    Options map[string]interface{} `json:\"options\"`\n}\n\ntype CustomRestResponse struct {\n    // Model is the model name that generated the response.\n    Model string `json:\"model\"`\n\n    // CreatedAt is the timestamp of the response.\n    CreatedAt time.Time `json:\"created_at\"`\n\n    // Response is the textual response itself.\n    Response string `json:\"response\"`\n}\n</code></pre>"},{"location":"tutorials/custom-rest-backend/#implementing-a-simple-rag","title":"Implementing a simple RAG","text":"<p>Now, we will build the RAG pipeline using <code>langchaingo</code>. The RAG application will query a knowledge base stored in <code>Qdrant</code> and use a large language model from <code>ollama</code> to generate responses. First, ensure that you have <code>ollama</code> and <code>Qdrant</code> running locally.</p> <pre><code># run Ollama\nollama run llama3.1\n\n# run Qdrant\ndocker run -p 6333:6333 -p 6334:6334 \\\n    -v $(pwd)/qdrant_storage:/qdrant/storage:z \\\n    qdrant/qdrant\n\n</code></pre> <p>We can download the <code>CNCF Q&amp;A dataset</code> from huggingface, and then load it into <code>Qdrant</code> using Python scribt below. </p> <pre><code>from langchain.embeddings import OllamaEmbeddings\nfrom langchain_community.document_loaders import CSVLoader\nfrom langchain_qdrant import QdrantVectorStore\n\nembeddings = OllamaEmbeddings(base_url=\"http://localhost:11434\", model=\"llama3.1\")\nloader = CSVLoader(file_path='./cncf_qa.csv', csv_args={\n    'delimiter': ',',\n    'quotechar': '\"',\n    'fieldnames': ['Question', 'Answer', 'Project', 'Filename', 'Subcategory', 'Category']\n})\ndata = loader.load()\nqdrant = QdrantVectorStore.from_documents(\n    data,\n    embeddings,\n    url=\"localhost:6333\",\n    prefer_grpc=False,\n    collection_name=\"my_documents\",\n)\n\ndata = loader.load()\n</code></pre> <p>Next, implement the RAG pipeline logic.</p> <pre><code>func rag(serverURL string, req CustomRestRequest) (string, error) {\n    model := req.Model\n    llm, err := ollama.New(ollama.WithServerURL(serverURL), ollama.WithModel(model))\n    if err != nil {\n        return \"\", err\n    }\n\n    embedder, err := embeddings.NewEmbedder(llm)\n    if err != nil {\n        return \"\", err\n    }\n\n    url, err := url.Parse(\"http://localhost:6333\")\n    if err != nil {\n        return \"\", err\n    }\n\n    // new a client of vector store\n    store, err := qdrant.New(\n        qdrant.WithURL(*url),\n        qdrant.WithCollectionName(\"my_documents\"),\n        qdrant.WithEmbedder(embedder),\n        qdrant.WithContentKey(\"page_content\"),\n    )\n    if err != nil {\n        return \"Wi\", err\n    }\n\n    optionsVector := []vectorstores.Option{\n        vectorstores.WithScoreThreshold(0.6),\n    }\n\n    retriever := vectorstores.ToRetriever(store, 10, optionsVector...)\n    errMessage := req.Options[\"message\"].(string)\n    // search local knowledge\n    resDocs, err := retriever.GetRelevantDocuments(context.Background(), errMessage)\n    if err != nil {\n        return \"\", err\n    }\n\n    // get content\n    x := make([]string, len(resDocs))\n    for i, doc := range resDocs {\n        x[i] = doc.PageContent\n    }\n\n    // generate content by LLM\n    ragPromptTemplate := `Base on context: %s;\n    Please generate a response to the following query and response doesn't include context, if context is empty, generate a response using the model's knowledge and capabilities: \\n %s`\n    prompt := fmt.Sprintf(ragPromptTemplate, strings.Join(x, \"; \"), req.Prompt)\n    ctx := context.Background()\n    completion, err := llms.GenerateFromSinglePrompt(ctx, llm, prompt)\n    if err != nil {\n        return \"\", err\n    }\n    fmt.Println(\"Error: \"+errMessage, \"Answer: \"+completion)\n    return completion, err\n}\n</code></pre>"},{"location":"tutorials/custom-rest-backend/#testing-it-out","title":"Testing it out","text":"<p>To test this with K8sGPT we need to add a <code>customrest</code> AI backend configuration to point to this RAG service. We can do this by running the following command:</p> <pre><code>./k8sgpt auth add --backend customrest --baseurl http://localhost:8090/completion --model llama3.1\n</code></pre> <p>This will add the custom RAG service to the list of available backend in the K8sGPT CLI. To explain the analysis results using the custom RAG pipeline we can run the following command:</p> <pre><code>./k8sgpt analyze --backend customrest --explain \n</code></pre>"},{"location":"tutorials/custom-rest-backend/#whats-next","title":"What's next?","text":"<p>Now you've got the basics of how to write a custom AI backend, you can extend this to use private dataset for knowledge retrieval. You can also build more complex AI pipelines to explain the result obtained from <code>Analyzers</code> and provide more detailed recommendations.</p>"},{"location":"tutorials/observability/","title":"Integrating Observability with K8sGPT","text":"<p>Enhance your Kubernetes observability by integrating Prometheus and Grafana with K8sGPT. Follow these steps to set up and visualize your cluster's insights:</p>"},{"location":"tutorials/observability/#prerequisites","title":"Prerequisites","text":"<ul> <li>Prometheus: Install using Helm via Prometheus Community Helm Charts.</li> <li>Grafana Dashboard: Ensure Grafana is installed and accessible in your environment.</li> </ul>"},{"location":"tutorials/observability/#installation-steps","title":"Installation Steps","text":"<p>Install the K8sGPT Operator with observability features enabled:</p> <pre><code>helm install release k8sgpt/k8sgpt-operator -n k8sgpt-operator-system --create-namespace --set interplex.enabled=true --set grafanaDashboard.enabled=true --set serviceMonitor.enabled=true\n</code></pre> <p>This command: - Creates a ServiceMonitor to integrate with Prometheus. - Automatically configures and populates data into your Grafana dashboard.</p> <p>Once set up, you can explore key metrics like: - Results identified by K8sGPT. - Operator workload details, providing insight into resource usage and efficiency.</p> <p>See example of a K8sGPT Grafana dashboard</p> <p></p>"},{"location":"tutorials/playground/","title":"K8sGPT Playground","text":"<p>If you want to try out K8sGPT, we highly suggest you to follow this Killercoda example:</p> <p>Link: K8sGPT CLI Tutorial</p> <p>This tutorial covers:</p> <ul> <li>Run a simple analysis and explore possible options</li> <li>Discover how AI works Explanation</li> <li>Stay on the down-low with the anonymize option (because we don't want any trouble with the feds)</li> <li>Filter resources like a boss</li> <li>Use Integrations</li> </ul>"},{"location":"tutorials/slack-integration/","title":"Integrate K8sGPT operator with Slack","text":""},{"location":"tutorials/slack-integration/#slack-prerequisites","title":"Slack prerequisites","text":"<ul> <li>Create a slack channel</li> <li>Create a slack app</li> <li>Enable and create an incoming webhook</li> <li>Copy the webhook URL value</li> </ul> <p>You can follow Slack's documentation to create the webhook</p>"},{"location":"tutorials/slack-integration/#configure-the-k8sgpt-operator","title":"Configure the K8sGPT operator","text":"<p>Install the operator with HELM  </p> <pre><code>helm repo add k8sgpt https://charts.k8sgpt.ai/\nhelm repo update\nhelm install release k8sgpt/k8sgpt-operator -n k8sgpt-operator-system --create-namespace\n</code></pre> <p>Create OpenAI's secret  </p> <pre><code>kubectl create secret generic k8sgpt-sample-secret --from-literal=openai-api-key=$OPENAI_TOKEN -n k8sgpt-operator-system\n</code></pre> <p>Last but not least, deploy your K8sGPT Custom Resource</p> <pre><code>kubectl apply -f - &lt;&lt; EOF\napiVersion: core.k8sgpt.ai/v1alpha1\nkind: K8sGPT\nmetadata:\n  name: k8sgpt-sample\n  namespace: k8sgpt-operator-system\nspec:\n  ai:\n    enabled: true\n    model: gpt-4o-mini\n    backend: openai\n    secret:\n      name: k8sgpt-sample-secret\n      key: openai-api-key\n  noCache: false\n  version: v0.3.8\n  sink:\n    type: slack\n    webhook: &lt;your webhook url&gt;\nEOF\n</code></pre>"},{"location":"tutorials/content-collection/content-collection/","title":"Content Collection","text":"<p>This section provides a collection of videos, blog posts and more on K8sGPT, posted on external sites.</p>"},{"location":"tutorials/content-collection/content-collection/#blogs","title":"Blogs","text":"<p>Have a look at the K8sGPT blog on the website.</p> <p>Additionally, here are several blogs created by the community:</p> <ul> <li>K8sGPT + LocalAI: Unlock Kubernetes superpowers for free! by Tyler Gillson</li> <li>K8sGPT: Simplifying Kubernetes Diagnostics with Natural Language Processing by Karan Singh</li> <li>Kubernetes + ChatGPT = K8sGpt by Vijul Patel</li> <li>ChatGPT for your Kubernetes Cluster \u2014 k8sgpt by Renjith Ravindranathan</li> <li>Using the Trivy K8sGPT plugin by Renjith Ravindranathan</li> </ul>"},{"location":"tutorials/content-collection/content-collection/#videos","title":"Videos","text":"<ul> <li>Kubernetes + OpenAI = K8sGPT, giving you AI superpowers!</li> <li>k8sgpt Getting Started (2023)</li> <li>Debugging Kubernetes with AI: k8sGPT || AI-Powered Debugging for Kubernetes</li> </ul>"},{"location":"tutorials/content-collection/content-collection/#contributing","title":"Contributing","text":"<p>If you have created any content around K8sGPT, then please add it to this collection.</p>"}]}